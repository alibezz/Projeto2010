acl06dist2
Encontrado pq tava linkado na survey de pang e lee

Duas coleções de documentos: A e B. Calcula-se o grau de contraste das coleções pela distância entre A e B. Distinguir coleções com perspectivas diferentes de outros tipos de coleções.

*Esse artigo traz uma definição de perspectiva do dicionário! Isso é legal!!*

O related work dele volta MUUUUUITOOOO no tempo! "How beliefs and ideologies can be represented in computers".

Esse artigo usa um *statistical approach* EM VEZ de uma *knowledge base construída manualmente*. *statistical approach* => statistical regularities of any pairs of document collections with opposing perspectives.

O problema: DUAS CLASSES DE DOCUMENTOS SÃO DE PERSPECTIVAS DIFERENTES? Não é modelar as perspectivas per se, saca? É diferente do problema que eles tratam em 2006, que é de classificar documentos de acordo com suas perspectvas.

Usa os documentos como vetores em um espaço V-dimensional. Bags of words, saca? Isso tira mto das estruturas sintáticas/semânticas das coisas.

*Bag-of-word representation, tão comum em data retriveal, é um critério.*

Tipo assim: como os documentos são representados.

theta => |theta| = V; theta ~ Dirichlet(alpha); documento yi ~ Multinomial(ni, theta). O que ele quer é comparar os parâmetros theta. 

p(theta|A) = Dirichlet(theta|alpha+sum(yi; yi pert. A))

Calcula a Kullback-Leibler de forma aproximada pra essas distribuições posteriores, usando método de Monte Carlo. Isso pra medir a divergência entre duas distribuições.

Compara os corpora do bitterlemons e o 2004 Presidential Debate com un Reuters da vida. Pré-processamento: NO STEMMING, NO STOP WORDS.

O método dos experimentos: o cara cria 4 categorias, same topic, different topic, same perspective, different perspective. Ele mede a qualidade de usar KL assim: quão menor for a cadeiaa da divergencia da distribuição, mais afinado é o teste para dif. perspectivas (?).

Usa um intervalo de 95% de confiança para as estimativas com Monte Carlo para a divergência KL. 


KL pequena: mesmo tópico ou perspectiva.
KL média: mesmo tópico, diferente perspectiva.
KL grande: tópicos diferentes.

Será que essa KL média tem a ver com estilo de escrita? Dois editores só, no caso do bitterlemons, e só duas pessoas com as eleições. Bom, ele testa com os sets (Israeli guests. palestinian guests), escritos por mais de 200 pessoas diferentes, e vê se a KL tb é média. Isso reforça a chance de que oq tá sendo identificado é mesmo perspectiva - não writing styles. O resultado é ótimo.

Assumption básica dessa galera que usa bag-of-words pra perspective mining: authors of different perspectives write or speak in a similar vocabulary, but with emphasis in different words.

É bacana como ele mostra que o Delta(theta), que é a E[theta|A] - E[theta|B], quando versus E[theta|A], dispersa cada vez mais do zero conforme vai mudando de Same Topic -> Same Perspective -> Different Perspective -> Different Topic. De onde essas diferenças vêm? *De que palavras são usadas e da ênfase nelas.* Essa é a raiz do teste funcionar. Será que essa hipótese sempre é válida?


Problema em aberto: testar para different text genres (Kessler et al., 1997). Nem sempre vai se comportar bem assim; mas é um teste.


Esse artigo chega a conclusões mto bestas! Honestamente? Considerando a bag of words e a hipótese básica assumida por todos esses modelos, é claro que usando KL em theta ele chegaria a esses resultados! Claro q tópicos diferentes usam palavras diferentes...


